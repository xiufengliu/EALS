# KDD-Quality Experimental Framework for Dynamic Information Lattices

This framework provides comprehensive experimental evaluation suitable for top-tier conference submission (KDD). It includes all 13 processed datasets, multiple baseline comparisons, statistical significance testing, and reproducibility features.

## ðŸ“Š Datasets (13 Total)

### Large-Scale Datasets
- **electricity**: 26,304 Ã— 320 series (hourly electricity consumption)
- **traffic**: 17,544 Ã— 861 series (hourly traffic occupancy)
- **ecl**: 140,256 Ã— 13 series (electricity consuming load)

### Medium-Scale Datasets  
- **ettm1**: 69,680 Ã— 17 series (15-minute electricity transformer)
- **ettm2**: 69,680 Ã— 17 series (15-minute electricity transformer)
- **weather**: 52,696 Ã— 20 series (10-minute weather measurements)
- **solar**: 52,560 Ã— 137 series (10-minute photovoltaic power)

### Small-Scale Datasets
- **etth1**: 17,420 Ã— 17 series (hourly electricity transformer)
- **etth2**: 17,420 Ã— 17 series (hourly electricity transformer)
- **exchange_rate**: 7,588 Ã— 7 series (daily exchange rates)
- **southern_china**: 679 Ã— 441 series (regional data)
- **gefcom2014**: 528 Ã— 139 series (energy forecasting competition)
- **illness**: 966 Ã— 6 series (weekly influenza surveillance)

## ðŸ”¬ Experimental Design

### Baseline Methods (15 Total)
1. **Simple Baselines**: Naive seasonal, Linear trend, ARIMA
2. **Deep Learning**: LSTM, Transformer, Informer, Autoformer, FEDformer, PatchTST
3. **Recent SOTA**: TimesNet, DLinear, N-BEATS
4. **Diffusion Methods**: TSDiff, CSDI, TimeGrad

### Evaluation Metrics
- **Point Forecast**: MAE, RMSE, MAPE, SMAPE, MASE
- **Probabilistic**: CRPS, Quantile Loss, Energy Score, Coverage Probability
- **Distribution**: Wasserstein Distance, KL Divergence
- **Efficiency**: Training Time, Inference Time, Memory Usage, Energy Consumption
- **Robustness**: Noise Robustness, Missing Data Robustness

### Statistical Validation
- **5-fold cross-validation** with multiple random seeds
- **Wilcoxon signed-rank tests** for pairwise comparisons
- **Friedman test** for multiple method comparison
- **Bonferroni correction** for multiple testing
- **Effect size analysis** (Cohen's d)

## ðŸš€ Running Experiments

### 1. Quick Test (Single Dataset)
```bash
# Test on small dataset
python train_multi_dataset.py --dataset illness --epochs 10 --sequence_length 24 --prediction_length 6

# Test baseline
python baselines/run_baseline.py --method dlinear --dataset illness --epochs 10
```

### 2. Full KDD Experimental Suite
```bash
# Dry run to see what would be submitted
python kdd_experimental_framework.py --dry-run

# Submit all experiments (WARNING: This submits ~1000+ jobs)
python kdd_experimental_framework.py --max-concurrent 20

# Submit specific scales only
python kdd_experimental_framework.py --scales small_scale medium_scale
```

### 3. Monitor Progress
```bash
# Check job status
bjobs | grep kdd_

# Monitor specific experiment
tail -f logs/kdd_dil_electricity_96_24_s42_f0.out

# Kill all experiments if needed
bjobs | grep kdd_ | awk '{print $1}' | xargs bkill
```

## ðŸ“ˆ Analysis and Results

### 1. Collect and Analyze Results
```bash
# Analyze experimental results
python analyze_experimental_results.py --experiments-dir experiments/kdd --stats --plots --latex

# Statistical significance testing
python statistical_analysis.py --results-dir experiments/kdd --output-dir statistical_analysis
```

### 2. Generate KDD Tables
```bash
# Generate LaTeX tables for paper
python generate_kdd_tables.py --results-dir experiments/kdd --output-dir paper_tables
```

## ðŸ“‹ Experimental Configuration

### Resource Requirements
- **Small datasets**: 8-16GB GPU memory, 8 hours
- **Medium datasets**: 16-24GB GPU memory, 12 hours  
- **Large datasets**: 24-32GB GPU memory, 24 hours
- **Very large datasets**: 32-40GB GPU memory, 24+ hours

### Reproducibility Features
- **Fixed random seeds**: 42, 123, 456, 789, 999
- **Deterministic operations**: CUDA deterministic mode
- **Version control**: All dependencies pinned
- **Environment**: Conda/Docker containers available
- **Hardware specification**: Documented GPU/CPU requirements

## ðŸ“Š Expected Results Structure

```
experiments/kdd/
â”œâ”€â”€ small_scale/
â”‚   â”œâ”€â”€ illness/
â”‚   â”‚   â”œâ”€â”€ dil/
â”‚   â”‚   â”‚   â”œâ”€â”€ best_model.pth
â”‚   â”‚   â”‚   â””â”€â”€ training_log.json
â”‚   â”‚   â”œâ”€â”€ dlinear/
â”‚   â”‚   â””â”€â”€ patchtst/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ medium_scale/
â”œâ”€â”€ large_scale/
â””â”€â”€ very_large_scale/

statistical_analysis/
â”œâ”€â”€ statistical_analysis_report.txt
â”œâ”€â”€ pairwise_comparisons.csv
â”œâ”€â”€ effect_sizes.csv
â”œâ”€â”€ friedman_test.json
â””â”€â”€ plots/
    â”œâ”€â”€ method_comparison_boxplot.png
    â””â”€â”€ significance_matrix_*.png

paper_tables/
â”œâ”€â”€ main_results_table.tex
â”œâ”€â”€ ablation_study_table.tex
â”œâ”€â”€ scalability_analysis_table.tex
â””â”€â”€ statistical_significance_table.tex
```

## ðŸŽ¯ KDD Submission Checklist

### Experimental Rigor
- âœ… **Comprehensive baselines**: 15 state-of-the-art methods
- âœ… **Multiple datasets**: 13 diverse real-world datasets
- âœ… **Statistical validation**: Proper significance testing
- âœ… **Cross-validation**: 5-fold CV with multiple seeds
- âœ… **Effect size analysis**: Cohen's d for practical significance
- âœ… **Multiple metrics**: Beyond simple MSE/MAE

### Reproducibility
- âœ… **Code availability**: Complete implementation provided
- âœ… **Data preprocessing**: Standardized pipeline
- âœ… **Hyperparameters**: All settings documented
- âœ… **Random seeds**: Fixed for reproducibility
- âœ… **Environment**: Docker/Conda specifications
- âœ… **Hardware requirements**: Clearly specified

### Presentation Quality
- âœ… **Professional tables**: LaTeX formatted results
- âœ… **Statistical significance**: Properly marked in tables
- âœ… **Visualization**: High-quality plots and figures
- âœ… **Error analysis**: Confidence intervals and error bars
- âœ… **Scalability analysis**: Performance vs dataset size
- âœ… **Ablation studies**: Component contribution analysis

## ðŸ”§ Troubleshooting

### Common Issues
1. **Out of memory**: Reduce batch size or use gradient accumulation
2. **Job failures**: Check GPU availability and resource limits
3. **Missing results**: Verify job completion and output directories
4. **Statistical tests fail**: Ensure sufficient data points per method

### Performance Optimization
- Use mixed precision training for large models
- Implement gradient checkpointing for memory efficiency
- Use data parallel training for very large datasets
- Cache preprocessed data to reduce I/O overhead

## ðŸ“š References

This experimental framework follows best practices from:
- NeurIPS/ICML experimental guidelines
- Time series forecasting benchmarking standards
- Statistical significance testing in ML conferences
- Reproducibility checklists for top-tier venues

For questions or issues, please refer to the detailed documentation in each script or contact the development team.
